#!/usr/bin/env python
# coding: utf-8

# # Regression Predict Student Solution
# 
# © Explore Data Science Academy
# 
# ---
# ### Honour Code
# 
# I {**Fikile Lubambo**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).
# 
# Non-compliance with the honour code constitutes a material breach of contract.
# 
# ### Predict Overview: Spain Electricity Shortfall Challenge
# 
# The government of Spain is considering an expansion of it's renewable energy resource infrastructure investments. As such, they require information on the trends and patterns of the countries renewable sources and fossil fuel energy generation. Your company has been awarded the contract to:
# 
# - 1. analyse the supplied data;
# - 2. identify potential errors in the data and clean the existing data set;
# - 3. determine if additional features can be added to enrich the data set;
# - 4. build a model that is capable of forecasting the three hourly demand shortfalls;
# - 5. evaluate the accuracy of the best machine learning model;
# - 6. determine what features were most important in the model’s prediction decision, and
# - 7. explain the inner working of the model to a non-technical audience.
# 
# Formally the problem statement was given to you, the senior data scientist, by your manager via email reads as follow:
# 
# > In this project you are tasked to model the shortfall between the energy generated by means of fossil fuels and various renewable sources - for the country of Spain. The daily shortfall, which will be referred to as the target variable, will be modelled as a function of various city-specific weather features such as `pressure`, `wind speed`, `humidity`, etc. As with all data science projects, the provided features are rarely adequate predictors of the target variable. As such, you are required to perform feature engineering to ensure that you will be able to accurately model Spain's three hourly shortfalls.
#  
# On top of this, she has provided you with a starter notebook containing vague explanations of what the main outcomes are. 

# <a id="cont"></a>
# 
# ## Table of Contents
# 
# <a href=#one>1. Importing Packages</a>
# 
# <a href=#two>2. Loading Data</a>
# 
# <a href=#three>3. Exploratory Data Analysis (EDA)</a>
# 
# <a href=#four>4. Data Engineering</a>
# 
# <a href=#five>5. Modeling</a>
# 
# <a href=#six>6. Model Performance</a>
# 
# <a href=#seven>7. Model Explanations</a>

#  <a id="one"></a>
# ## 1. Importing Packages
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Importing Packages ⚡ |
# | :--------------------------- |
# | In this section you are required to import, and briefly discuss, the libraries that will be used throughout your analysis and modelling. |
# 
# ---

# In[1]:


# Libraries for data loading, data manipulation and data visulisation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.correlation import plot_corr
from scipy.stats import pearsonr
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import VarianceThreshold
# Libraries for data preparation and model building
#import *

# Setting global constants to ensure notebook results are reproducible
#PARAMETER_CONSTANT = ###


# It is also benificial to write a quick function in order to display the complete output of a dataframe that will be used throughout the notebook:

# In[2]:


def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')
    return


# <a id="two"></a>
# ## 2. Loading the Data
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Loading the data ⚡ |
# | :--------------------------- |
# | In this section you are required to load the data from the `df_train` file into a DataFrame. |
# 
# ---

# In[3]:


df = pd.read_csv("df_train.csv")


# <a id="three"></a>
# ## 3. Exploratory Data Analysis (EDA)
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Exploratory data analysis ⚡ |
# | :--------------------------- |
# | In this section, we will perform an in-depth analysis of all the variables in the DataFrame. **Note that this analysis will be repeated after the Data Engineering process is complete, in order to compare how these metrics are affected** |
# 
# ---
# 

# In[4]:


# Overview of dataframe: Basic Analysis - See notebook 3.2 on Athena (Univariate and Multivariate Analysis [Notebook])


# In[5]:


# Univariate Analysis: Non-Graphical - See notebook 3.2 on Athena (Univariate and Multivariate Analysis [Notebook])


# In[6]:


# Univariate Analysis: Graphical - See notebook 3.2 on Athena (Univariate and Multivariate Analysis [Notebook])


# # Multivariate Analysis: Non-Graphical 

# In[37]:


#Looking at the features that we are working with
df.head()


# In[39]:


#Finfing the number of rows and columns in the data
df.shape


# The shape command shows us that we have 8763 rows of data and 49 features.

# In[40]:


#Checking for datatype in our data
df.info()


# The info command confirms our categorical and numerical features. If a feature (variable) is categorical the Dtype is object and if it is a numerical variable the Dtype is an int64 or float64.

# Null values for each feature can also be checked by using the following command:
# 
# - df.isnull().sum()
#     
# This command will provide the total number of null values appearing in each feature.

# In[41]:


#Checking for the total number of null values in each feature of the data
df.isnull().sum()


# This confirms that there are 2068 null values in total and these are all in the feature "Valencia_pressure" of the data.

# For this analysis, we determined the relationship between any two numerical features by calculating the correlation coefficient. If two features have a strong positive correlation, it means that if the value of one feature increases, the value of the other feature also increases. The Pearson correlation measures the linear relationship between features and assumes that the features are normally distributed
# 
# The command we will use to determine the correlation between features is:
# 
# - df.corr()

# In[7]:


#Determining the correlation between features
df.corr()


# In[32]:


df.cov()


# In[8]:


# Multivariate Analysis: Graphical  - See notebook 3.2 on Athena (Univariate and Multivariate Analysis [Notebook])


# In[9]:


# Checking for Linearity - See notebook 6.4 on Athena (Multiple Linear Regression - Advanced Regression Analysis [Notebook])


# In[10]:


# Checking for Multicollinearity - See notebook 6.4 on Athena (Multiple Linear Regression - Advanced Regression Analysis [Notebook])


# ### 3.8  Correlations between features
# 
# Using the inbuilt functions of the *pandas* module, we can easily investigate the correlations between features by setting up a correlation matrix:

# In[11]:


# Investigate feature correlations 

df.corr()


# Since we are looking at the correlation between 46 variables (which includes our dependent variable), it would be more benificial to draw a heatmap of correlation to easily identify strong correlations between variables   

# In[12]:


#Using a heatmap to investigate feature correlations:

fig = plt.figure(figsize=(15,15));
ax = fig.add_subplot(111);
plot_corr(df.corr(), xnames = df.corr().columns, ax = ax);


# From looking at the correlation heatmap, we see that there are a large amount of strong correlations, both positive and negative, between variables in the dataset. For example: `Seville_humidity` and `Seville_temp` are very strongly neagtivley correlated, while `Valencia_temp` and `Madrid_temp` are very strongly positively correlated. This means that our dataset contains a lot of superfluous and redundant information, which will negatively impact the predictive power of the model. We will either have to select variables to include in the model manually, or make use of a regression model that automatically scales the coefficients, like a **Ridge** or **LASSO** regression model.
# 
# If we choose to include and exclude predictor variables manually, we would need to establish some criteria to apply our filtering of the data. A common way of doing this is to examine the correlation between our predictor variables, and our dependent variable `load_shortfall_3h`. We do this by using the Pearson Regression module contained in the Scipy package. This requires that no *NAN* values exist in our dataset, so just for the moment, we exclude the `Valencia_pressure` column.

# In[13]:


#The `Valencia_pressure` column is the only one containing NAN values
print_full(df.isnull().sum())


# In[14]:


corrs = df.drop("Valencia_pressure", axis="columns").corr()['load_shortfall_3h'].sort_values(ascending=False)

# Using the Pearson regression module included in the SciPy package, we can build a dictionary of correlation coefficients and p-values

dict_cp = {}

column_titles = [col for col in corrs.index if col!= 'load_shortfall_3h']
for col in column_titles:
    p_val = round(pearsonr(df[col], df['load_shortfall_3h'])[1],6)
    dict_cp[col] = {'Correlation_Coefficient':corrs[col],
                    'P_Value':p_val}


df_cp = pd.DataFrame(dict_cp).T
df_cp_sorted = df_cp.sort_values('P_Value')
df_cp_sorted[df_cp_sorted['P_Value']<0.1]


# NOTE TO KOBUS --- EXPAND THIS SECTION

# ### 3.9  Variance of features
# 
# 
# Another way of choosing which variables to manually include in the model, is to remove features whose values don't change much between observations, since they don't contribute much information to the model. We therefore select a variance threshold, and remove all features below this threshold. 
# 
# Since variance is dependent on scale, we will first need to normalize the features:

# In[15]:


# Seperate dependent variable (y) from independent variables (X)

X_names = list(df.columns)
y_name = "load_shortfall_3h"
X_names.remove(y_name)


# In[16]:


# investigate column types

print(df.info())



# In[17]:


#Remove non-numeric columns (THIS IS DONE ONLY FOR NOW. THESE COLUMNS WILL BE ENGINEERED IN THE DATA ENGINEERING SECTION, AND WILL BE INCLUDED IN THE SUBSEQUENT EDA)

X_names.remove("time")
X_names.remove("Valencia_wind_deg")
X_names.remove("Seville_pressure")


# In[18]:


X_data = df[X_names]
y_data = df[y_name]

# Normalizing the data (using MinMaxScaler in sklearn.preprocessing package):
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_data)
X_normalize = pd.DataFrame(X_scaled, columns=X_data.columns)


# We can then implement an instance of the VarianceThreshold class contained in sklearn.feature_selection to select the correct subset of our features based on our chosen threshold (0.03 in this case): 

# In[19]:


# Create VarianceThreshold object
selector = VarianceThreshold(threshold=0.03)

# Use the object to apply the threshold on data
selector.fit(X_normalize)


# Having applied this threshold to the data, we can view the calculated variance for each predictor variable:

# In[20]:


# Get column variances
column_variances = selector.variances_

# Create dictionary of column variances
vars_dict = {}
vars_dict = [{"Variable_Name": c_name, "Variance": c_var}
             for c_name, c_var in zip(X_normalize.columns, column_variances)]
df_vars = pd.DataFrame(vars_dict)
df_vars.sort_values(by='Variance', ascending=False)


# The above table shows the variance for each predictor variable. We can then inspect all variables before applying our threshold to see if doing so might exclude important predictors form our model
# 
# Now, we can select our new columns to include in the model:

# In[21]:


# Select new columns
X_new = X_normalize[X_normalize.columns[selector.get_support(indices=True)]]

# Save variable names for later
X_var_names = X_new.columns

# Comparing predictor varaible count before and after applying variance threshold:
print("Before threshold: ", len(df_vars.T.columns), " predictors")
print("After threshold: ", len(X_new.columns), " predictors")


# NOTE FOR KOBUS --- INSERT CONCLUSION

# <a id="four"></a>
# ## 4. Data Engineering
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Data engineering ⚡ |
# | :--------------------------- |
# | In this section you are required to: clean the dataset, and possibly create new features - as identified in the EDA phase. |
# 
# ---

# In[22]:


# remove missing values/ features


# In[23]:


# create new features


# In[24]:


# engineer existing features


# <a id="five"></a>
# ## 5. Revisiting EDA with new features
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 

# <a id="six"></a>
# ## 6. Modelling
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Modelling ⚡ |
# | :--------------------------- |
# | In this section, you are required to create one or more regression models that are able to accurately predict the thee hour load shortfall. |
# 
# ---

# In[25]:


# split data


# In[26]:


# create targets and features dataset


# In[27]:


# create one or more ML models


# In[28]:


# evaluate one or more ML models


# <a id="seven"></a>
# ## 7. Model Performance
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Model performance ⚡ |
# | :--------------------------- |
# | In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |
# 
# ---

# In[29]:


# Compare model performance


# In[30]:


# Choose best model and motivate why it is the best choice


# <a id="eight"></a>
# ## 8. Model Explanations
# <a class="anchor" id="1.1"></a>
# <a href=#cont>Back to Table of Contents</a>
# 
# ---
#     
# | ⚡ Description: Model explanation ⚡ |
# | :--------------------------- |
# | In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |
# 
# ---

# In[31]:


# discuss chosen methods logic

